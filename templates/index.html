<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aprendizaje por Refuerzo</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='index.css') }}">
</head>
<body>
    <nav class="navbar">
        <div class="nav-content">
            <h2>Reinforcement Learning</h2>
            <a href="/agente" class="nav-btn">Demo Interactiva</a>
        </div>
    </nav>

    <main class="content">
        <section class="hero">
            <h1>Aprendizaje por Refuerzo</h1>
            <p class="subtitle">Un paradigma del aprendizaje autom谩tico donde un agente aprende a tomar decisiones mediante la interacci贸n con un entorno</p>
        </section>

        <section class="section">
            <p class="intro">
                El Aprendizaje por Refuerzo (Reinforcement Learning) es un paradigma del aprendizaje autom谩tico en el que un agente aprende a tomar decisiones mediante la interacci贸n con un entorno. Seg煤n Sutton & Barto (2015), el agente recibe recompensas en funci贸n de sus acciones y busca maximizar la suma acumulada de estas recompensas a lo largo del tiempo. A diferencia de otros enfoques, no aprende a partir de ejemplos etiquetados, sino por medio de ensayo y error.
            </p>
        </section>

        <section class="section">
            <h2>1. 驴En qu茅 se diferencia de otros paradigmas?</h2>
            <p>
                El aprendizaje supervisado utiliza datos etiquetados; el no supervisado descubre patrones sin etiquetas. En cambio, el aprendizaje por refuerzo no opera con datos est谩ticos: el agente genera su propia experiencia interactuando con el entorno. No recibe la acci贸n correcta como gu铆a, sino 煤nicamente se帽ales de recompensa, a menudo incompletas o retardadas, y debe descubrir qu茅 acciones maximizan el retorno a largo plazo.
            </p>
        </section>

        <section class="section">
            <h2>2. Componentes del modelo de Reinforcement Learning</h2>
            <p>
                El proceso de RL se basa en una relaci贸n continua entre agente y entorno. En cada paso, el agente observa el estado, toma una acci贸n seg煤n su pol铆tica y recibe una recompensa junto con el nuevo estado. Este ciclo se repite, y el objetivo es aprender una pol铆tica que maximice las recompensas acumuladas.
            </p>

            <div class="components-grid">
                <div class="component-card">
                    <h3>Agente</h3>
                    <p>Es quien toma decisiones. Puede ser un robot, un programa de trading o un controlador de tr谩fico. Recibe observaciones, procesa la informaci贸n y elige acciones buscando maximizar la recompensa futura.</p>
                </div>

                <div class="component-card">
                    <h3>Entorno</h3>
                    <p>Representa "el mundo" con el que el agente interact煤a. Determina c贸mo cambian los estados y qu茅 recompensas resultan de cada acci贸n.</p>
                </div>

                <div class="component-card">
                    <h3>Estado</h3>
                    <p>Es la informaci贸n relevante del entorno en un momento dado. Puede corresponder a variables f铆sicas, configuraciones de un sistema, un tablero de juego o representaciones generadas por redes neuronales.</p>
                </div>

                <div class="component-card">
                    <h3>Acci贸n</h3>
                    <p>Es la decisi贸n que ejecuta el agente dentro del entorno. Puede pertenecer a un espacio discreto o continuo. Su elecci贸n depende del objetivo del agente y de su pol铆tica.</p>
                </div>

                <div class="component-card">
                    <h3>Recompensa</h3>
                    <p>Es una se帽al escalar que indica qu茅 tan buena o mala fue la acci贸n tomada. Aunque puede ser escasa o retardada, es la gu铆a fundamental para el aprendizaje del agente.</p>
                </div>

                <div class="component-card">
                    <h3>Pol铆tica</h3>
                    <p>Es la regla que define qu茅 acci贸n tomar en cada estado. Puede ser determin铆stica o estoc谩stica. La pol铆tica es el n煤cleo del comportamiento del agente.</p>
                </div>
            </div>
        </section>

        <section class="section">
            <h2>3. Ciclo de aprendizaje del agente</h2>
            <p>El entrenamiento en RL se basa en tres conceptos fundamentales:</p>

            <div class="concept-list">
                <div class="concept-item">
                    <h3>Exploraci贸n vs Explotaci贸n</h3>
                    <p>Explorar significa probar nuevas acciones para descubrir mejores recompensas; explotar implica elegir la mejor acci贸n conocida. Un exceso de exploraci贸n lleva a acciones sub贸ptimas; un exceso de explotaci贸n puede estancar al agente en pol铆ticas no 贸ptimas.</p>
                </div>

                <div class="concept-item">
                    <h3>Retorno Acumulado</h3>
                    <p>Es la suma de las recompensas que el agente espera recibir desde un instante hasta el final del episodio. El objetivo es maximizar este retorno total.</p>
                </div>

                <div class="concept-item">
                    <h3>Descuento Temporal</h3>
                    <p>Para valorar menos las recompensas lejanas y estabilizar el aprendizaje, se usa un factor gamma (纬) que reduce el peso de recompensas futuras.</p>
                </div>
            </div>
        </section>

        <section class="section">
            <h2>4. Principales algoritmos de RL</h2>

            <div class="algorithm-list">
                <div class="algorithm-item">
                    <h3>Q-Learning</h3>
                    <p>Es un m茅todo off-policy que aprende el valor 贸ptimo de cada par estado-acci贸n sin seguir necesariamente la pol铆tica usada durante la exploraci贸n. Se emplea en entornos con espacios de estado peque帽os, juegos y problemas de control simples.</p>
                </div>

                <div class="algorithm-item">
                    <h3>SARSA</h3>
                    <p>Es un m茅todo on-policy: actualiza los valores seg煤n la acci贸n efectivamente tomada por la pol铆tica del agente, no seg煤n la mejor posible. Se usa cuando la exploraci贸n puede ser riesgosa y se busca un comportamiento m谩s seguro.</p>
                </div>

                <div class="algorithm-item">
                    <h3>Deep Q-Network (DQN)</h3>
                    <p>Extiende Q-Learning utilizando redes neuronales profundas para aproximar la funci贸n Q. Es ideal en entornos complejos, como aquellos representados por im谩genes o variables de alta dimensi贸n.</p>
                </div>
            </div>
        </section>

        <section class="section">
            <h2>5. Buenas pr谩cticas en Reinforcement Learning</h2>

            <div class="practices-list">
                <div class="practice-item">
                    <h3>Estabilidad del aprendizaje</h3>
                    <p>En m茅todos profundos como DQN, es com煤n que las estimaciones sean inestables. Se recomienda el uso de replay buffer para romper la correlaci贸n entre experiencias, y regularizaci贸n por entrop铆a para evitar que la pol铆tica se vuelva determinista demasiado pronto.</p>
                </div>

                <div class="practice-item">
                    <h3>Tasa de exploraci贸n</h3>
                    <p>Usar estrategias de exploraci贸n decreciente como 蔚-greedy permite que el agente explore ampliamente al inicio y luego explote conocimiento adquirido. Tambi茅n existen t茅cnicas modernas que separan pol铆ticas de exploraci贸n y explotaci贸n, como Decoupled RL.</p>
                </div>

                <div class="practice-item">
                    <h3>Manejo de recompensas</h3>
                    <p>El uso de reward shaping puede guiar al agente con recompensas intermedias para acelerar el aprendizaje; sin embargo, debe dise帽arse cuidadosamente para evitar que el agente optimice una meta equivocada.</p>
                </div>

                <div class="practice-item">
                    <h3>Convergencia y generalizaci贸n</h3>
                    <p>Es crucial ajustar hiperpar谩metros como tasa de aprendizaje, factor de descuento, tama帽o de batch y frecuencia de actualizaci贸n de redes objetivo. Entrenar en m煤ltiples variaciones del entorno mejora la generalizaci贸n y evita que la pol铆tica se sobreajuste.</p>
                </div>
            </div>
        </section>

        <footer class="footer">
            <p>Basado en Sutton & Barto (2015), GeekForGeeks (2025), OpenAI Spinning Group, y otras fuentes acad茅micas.</p>
            <p>Universidad de Cundinamarca | Dylan B. | Mayra C. | Miguel G. | Pedro Casas </p>
        </footer>
    </main>

    <div class="floating-demo" id="floatingDemo">
        <button class="close-btn" onclick="closeDemo()">&times;</button>
        <div class="demo-content">
            <div class="demo-icon"></div>
            <h3>Buscador de Tesoros</h3>
            <p>Entrena un agente inteligente usando Q-Learning en tiempo real</p>
            <a href="/agente" class="demo-btn">Probar Demo</a>
        </div>
    </div>

    <script>
        window.addEventListener('scroll', function() {
            const demo = document.getElementById('floatingDemo');
            if (window.scrollY > 300) {
                demo.classList.add('visible');
            }
        });

        function closeDemo() {
            document.getElementById('floatingDemo').style.display = 'none';
        }
    </script>
</body>
</html>
